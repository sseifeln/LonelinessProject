{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile as zp\n",
    "import pandas as pd\n",
    "from pypac import PACSession as Session #or use requests below if non-ONS\n",
    "#from requests import Session\n",
    "from io import BytesIO\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Summary Prescription Data\n",
    "\n",
    "This code has been created to work with 2016, 2017 and 2018 data, earlier data may have format inconsistencies that need to be dealt with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set File Location of Prescribing Data\n",
    "\n",
    "These files are the \"GP practice prescribing data - Presentation level\" zip files that come from: https://data.gov.uk/dataset/176ae264-2484-4afe-a297-d51798eb8228/gp-practice-prescribing-data-presentation-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to zip files\n",
    "path = r\"C:\\Users\\[your username]\\Loneliness\\Raw_Data\\\\\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in drug data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get drug data (NB some drugs duplicated for illnesses)\n",
    "drug_data = pd.read_csv(path[:-10] + \"drug_list.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Prescribing Data\n",
    "\n",
    "This code iterates over the monthly prescribing data, ultimately producing an aggregate table.\n",
    "\n",
    "Note, if you want to find prescribing for non-loneliness relatived diseases, all you need to do is provide a different set of drug_data and edit the code_loneliness function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find loneliness related prescribing\n",
    "def code_loneliness(x):\n",
    "    out = {}\n",
    "    # coding by illness categories\n",
    "    for illness in drug_data['illness'].unique():\n",
    "        out[illness] = x['BNF NAME'].str.contains(\"|\".join(drug_data[drug_data['illness'] == illness]['medication']),\n",
    "                                                  case=False, \n",
    "                                                  regex=True).astype('int16')\n",
    "    # Make dataframe\n",
    "    out = pd.DataFrame(out)\n",
    "    # Add loneliness related disease binary - avoids double counting some drugs.\n",
    "    out['loneliness'] = x['BNF NAME'].str.contains(\"|\".join(drug_data['medication'].unique()),\n",
    "                                                   case = False, \n",
    "                                                   regex = True).astype('int16')\n",
    "    # Return dataframe multiplied by counts of items.\n",
    "    return out.multiply(x['ITEMS'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dictionary for aggregation\n",
    "agg_cols = {col : 'sum' for col in drug_data['illness'].unique()}\n",
    "agg_cols['ITEMS'] = 'sum'\n",
    "agg_cols['loneliness'] = 'sum'\n",
    "for key in ['Date','SHA','PCT','pcstrip','CenterName','Street','Town','Town2','Postcode']:\n",
    "    agg_cols[key] = 'first'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_data = []\n",
    "\n",
    "for file in os.listdir(path):\n",
    "    with zp.ZipFile(path + file) as zipf:\n",
    "        zip_names = zipf.namelist()\n",
    "\n",
    "        # Deal with Address Files\n",
    "        addr_name = next((filename for filename in zip_names if \"ADDR\" in filename), None)\n",
    "        # Open address file in pandas, set header.\n",
    "        addr = pd.read_csv(zipf.open(addr_name), \n",
    "                           header=0, \n",
    "                           names = [\"Date\", \"PracCode\", \"PracName\",\"CenterName\",\n",
    "                                    \"Street\", \"Town\", \"Town2\", \"Postcode\"], \n",
    "                           usecols = range(8))\n",
    "\n",
    "        # Deal with prescription info\n",
    "        prescribe_name = next((filename for filename in zip_names if \"PDPI\" in filename), None)\n",
    "        # Open prescribing files in pandas.\n",
    "        prescribe = pd.read_csv(zipf.open(prescribe_name))\n",
    "        prescribe.columns = prescribe.columns.str.strip()\n",
    "        # Get counts of prescribing dataframe for loneliness related diseases\n",
    "        loneliness_prescribing = code_loneliness(prescribe[['BNF NAME','ITEMS']])\n",
    "        # merge dataframes\n",
    "        prescribe = prescribe.merge(loneliness_prescribing, left_index=True, right_index=True)\n",
    "        del loneliness_prescribing\n",
    "        \n",
    "        # merge in address information\n",
    "        prescribe = prescribe.merge(addr, left_on = 'PRACTICE', right_on = 'PracCode')\n",
    "        del addr\n",
    "        \n",
    "        # Create uniform postcode field\n",
    "        prescribe['pcstrip'] = prescribe['Postcode'].str.replace(\"\\s\",\"\")\n",
    "        \n",
    "        # get a summary - grouping by PracCode\n",
    "        summary = prescribe.groupby('PracCode', as_index=False).agg(agg_cols)\n",
    "        del prescribe\n",
    "        \n",
    "        monthly_data.append(summary)\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all the monthly data together.\n",
    "data = pd.concat(monthly_data, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save aggregated data\n",
    "data.to_csv(path[:-10] + \"processed_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Postcode Location\n",
    "\n",
    "Postcode location is pulled in from the latest ONS NSPL (National Statistics Postcode Lookup). There is a guide to fields here: http://geoportal.statistics.gov.uk/datasets/0a404beab6f544be8fb72d0c2b12d62b\n",
    "\n",
    "NB - If using an ONS laptop `pip install pypac`, if not comment pypac import above and use requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(path[:-10] + \"processed_data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in postcode lookup data\n",
    "# This is the persistent link to the latest ONS NSPL\n",
    "postcode_url = \"http://geoportal.statistics.gov.uk/datasets/055c2d8135ca4297a85d624bb68aefdb_0.csv\"\n",
    "\n",
    "with Session() as session:\n",
    "    response = session.get(postcode_url, verify = False)\n",
    "\n",
    "field_dtypes = {'objectid': 'int32', 'pcd':'str', 'pcd2': 'str', 'pcds':'str', 'dointr':'str','doterm':'str',\n",
    "                'usertype':'int8','oseast1m': 'float', 'osnorth1m': 'float', 'osgrdind':'int8', 'lat':'float', \n",
    "                'long':'float', 'X':'float', 'Y':'float', 'imd': 'int8',\n",
    "                'oa11':'str', 'cty': 'str', 'ced':'str', 'laua': 'str', 'ward': 'str', 'hlthau':'str',\n",
    "                'ctry': 'str','pcon': 'str','eer': 'str','teclec': 'str','ttwa': 'str','pct': 'str','nuts': 'str',\n",
    "                'park': 'str','lsoa11': 'str','msoa11': 'str','wz11': 'str','ccg': 'str','bua11': 'str',\n",
    "                'buasd11': 'str','ru11ind': 'str','oac11': 'str','lep1': 'str','lep2': 'str','pfa': 'str',\n",
    "                'ced': 'str','nhser': 'str','rgn': 'str','calncv': 'str','stp': 'str'}\n",
    "\n",
    "pc = pd.read_csv(BytesIO(response.content), dtype = field_dtypes)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pcstrip for matching\n",
    "pc['pcstrip'] = pc['pcd'].str.replace(\"\\s\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB - here I'm joining 2011 LSOA, 2011 MSOA, Rural-Urban Indicator, Region (formally GOR), Local Authority Area (effectively district), and IMD score (NB separate for each country). However, you can add any of the geography codes available in the NSPL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_temp = data.merge(pc[['pcstrip','oseast1m','osnrth1m','lsoa11','msoa11','ru11ind','rgn','laua','imd']], \n",
    "                       how = 'left',\n",
    "                       on = 'pcstrip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing postcodes\n",
    "data_temp[data_temp['oseast1m'].isnull()]['pcstrip'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Missing Postcodes - appear to be typos.\n",
    "new_pcs = {'DL154SB': 'DL54SB', 'WR59QT':'WR52QT', 'DN18QN':'DN48QN', 'HU32AE':'HU34AE','GL10QD': 'GL13NN',\n",
    "           'ME122TZ':'ME102TZ', 'BS378NG':'BS374NG', 'CV115PO':'CV115PQ', 'TW152EA':'TW153EA', 'EN24EJ':'EN80BX', \n",
    "           'YO302JS':'YO306JA','L62UN':'L67UN', 'NG698DB':'NG98DA', 'HA24ES':'HA14ES'}\n",
    "\n",
    "data['pcstrip'] = data['pcstrip'].map(new_pcs).fillna(data['pcstrip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data\n",
    "data = data.merge(pc[['pcstrip','oseast1m','osnrth1m','lsoa11','msoa11','ru11ind','rgn','laua','imd']], \n",
    "                  how = 'left', \n",
    "                  on = 'pcstrip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save aggregated data\n",
    "data.to_csv(path[:-10] + \"processed_data_with_postcodes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Postcodes \n",
    "\n",
    "Some Practice Codes have more than one postcode associated with them. Possible reasons for this are:\n",
    "* Practices move to a new location.\n",
    "* Practices are assigned a new postcode but don't physically move.\n",
    "* Practice postcodes are wrongly entered at a particular wave and subsequently fixed.\n",
    "\n",
    "There are 794 practices codes which have more than 1 postcode assigned to them, this is about 7% of unique practices.\n",
    "\n",
    "764 for those practices have 2 postcodes associated with them, while 30 have 3 postcodes.\n",
    "\n",
    "102 of these practices fall within the same LSOA, 669 fall within 2 different LSOAs, and 30 within 3 different LSOAs.\n",
    "\n",
    "We'll ignore this for now - this will require some more advanced cleaning - useful to be aware of though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for 1 postcode per Practice Code\n",
    "pc_prac_counts = data.groupby('PracCode')['pcstrip'].unique().map(len)\n",
    "# 794 practices have more than 1 postcode associated with it.\n",
    "pc_prac_counts[pc_prac_counts > 1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice codes with multiple associated postcodes account forc. 7% of the data\n",
    "pc_prac_counts[pc_prac_counts > 1].count()/len(pc_prac_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_prac_counts[pc_prac_counts > 1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of these Practices falling within the same LSOA\n",
    "(data[data['PracCode'].isin(pc_prac_counts[pc_prac_counts > 1].index)]\n",
    " .groupby('PracCode')['lsoa11']\n",
    " .unique()\n",
    " .map(len)\n",
    " .value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subsetting the Data\n",
    "\n",
    "## Use only General Practice surgeries\n",
    "\n",
    "Use the 'Patients Registered at a GP Practice' data from: https://digital.nhs.uk/data-and-information/publications/statistical/patients-registered-at-a-gp-practice to get GP surgery codes and subset the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(path[:-10] + \"processed_data_with_postcodes.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get GP files\n",
    "gp_path = r\"C:\\Users\\[your username]\\Loneliness\\GP Data\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_combine = []\n",
    "\n",
    "for file in os.listdir(gp_path):\n",
    "    # read file into pandas\n",
    "    month, year = file[-10:-4].split(\"-\")\n",
    "    \n",
    "    # Deal with different file structures\n",
    "    if (year == '16') or (year == '17' and month == 'jan'):\n",
    "        gp_data = pd.read_csv(gp_path + file)\n",
    "        gp_data['DATE'] = \"01\" + month.upper() + \"20\" + year\n",
    "        gp_data.columns = gp_data.columns.str.upper()\n",
    "        gp_data = gp_data.rename(columns = {'DATE':'Date','GP_PRACTICE_CODE':'PracCode','TOTAL_ALL':'NUMBER_OF_PATIENTS'})\n",
    "        gp_combine.append(gp_data[['Date','PracCode','POSTCODE','NUMBER_OF_PATIENTS']])\n",
    "    else:\n",
    "        gp_data = pd.read_csv(gp_path + file)\n",
    "        gp_data.columns = gp_data.columns.str.upper().str.replace(\" \",\"_\")\n",
    "        gp_data = gp_data.rename(columns = {'EXTRACT_DATE':'Date', 'CODE':'PracCode'})\n",
    "        if year == '17' and month == 'jun':\n",
    "            gp_data['Date'] = \"01\" + month.upper() + \"20\" + year\n",
    "        gp_combine.append(gp_data[['Date','PracCode','POSTCODE','NUMBER_OF_PATIENTS']])\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all the gp data together.\n",
    "gp_data = pd.concat(gp_combine, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique codes for GP surgeries and subset the prescribing data according to these codes.\n",
    "gp_ids = gp_data['PracCode'].unique()\n",
    "data = data[data['PracCode'].isin(gp_ids)].copy()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make date a datetime variable\n",
    "gp_data['Date'] = pd.to_datetime(gp_data['Date'], format = '%d%b%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make date a datetime variable - days are assigned as first day of the month.\n",
    "data['Date'] = pd.to_datetime(data['Date'], format = '%Y%m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on the basis of date and PracCode - produces some nulls for counts.\n",
    "# It may be possible to predict missing values using a time-series model.\n",
    "data = data.merge(gp_data, how = 'left', on = ['Date','PracCode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save aggregated data\n",
    "data.to_csv(path[:-10] + \"processed_data_with_postcodes_GPs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Statistics from Prescribing Counts\n",
    "\n",
    "## Percentages At Postcode Level\n",
    "\n",
    "Aggregate observations to postcodes and compute percentages for 'depression', 'alzheimers', 'blood pressure', 'hypertension', 'diabetes', 'cardiovascular disease', 'insomnia', 'addiction', 'social anxiety', and 'loneliness'.\n",
    "\n",
    "## Outlier Removal\n",
    "\n",
    "Should we remove some GPs on the basis that they have very low/high values which might indicate they are not accessible to the general population, and instead represent specialist services?\n",
    "\n",
    "Currently, we won't do this, but it's an advanced task to look into.\n",
    "\n",
    "## Standardise Percentages\n",
    "\n",
    "Should we standardise within time points, or across them? Or standardise with GP practices or across them?\n",
    "\n",
    "Can't standardize within GPs, as can't then compare between GPs.\n",
    "\n",
    "Can't standardise across GPs within years, as can't then compare between years.\n",
    "\n",
    "Can't standardise across GPs across years, as can't then disambiguate changes to rank order over time.\n",
    "\n",
    "<u> First Step </u>\n",
    "\n",
    "Take the average percentage of disease groups within postcodes annually - this is then an annual summary measure of prescribing by postcode. Aggregation entire depends on desired time frame for analysis.\n",
    "\n",
    "NB, this is a mean of percentages - could also calculate an overall percentage by summing monthly counts by year and dividing through by monthly sum of items.\n",
    "\n",
    "<u> Second Step </u>\n",
    "\n",
    "z-score standardise for earliest year observed across GPs. Store mean and standard deviation.\n",
    "\n",
    "z-score standardise subsequent years wrt baseline mean and standard deviation.\n",
    "\n",
    "OR\n",
    "\n",
    "Use min-max normalisation by year (decile normalisation?). This standardises the different percentages to the same range - although in theory they're comparable anyway...\n",
    "\n",
    "## Aggregation and Percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(path[:-10] + \"processed_data_with_postcodes_GPs.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dictionary for aggregation\n",
    "# counts to sum\n",
    "agg_cols = {col : 'sum' for col in drug_data['illness'].unique()}\n",
    "agg_cols['ITEMS'] = 'sum'\n",
    "agg_cols['loneliness'] = 'sum'\n",
    "agg_cols['NUMBER_OF_PATIENTS'] = 'sum'\n",
    "\n",
    "# Other data to preserve\n",
    "for key in ['SHA','PCT','Street','Town','Town2','Postcode','oseast1m', 'osnrth1m',\n",
    "            'lsoa11', 'msoa11','ru11ind', 'rgn', 'laua', 'imd']:\n",
    "    agg_cols[key] = 'first'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do aggregation and produce counts by postcode by date.\n",
    "data = data.groupby(['pcstrip','Date'], as_index=False).agg(agg_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate percentages\n",
    "perc_cols = drug_data['illness'].unique()\n",
    "target_cols = perc_cols + '_perc'\n",
    "\n",
    "# Percentages for discrete illness groups\n",
    "data[target_cols] = data[perc_cols].divide(data['ITEMS'], axis=0) * 100\n",
    "\n",
    "# Overall percentage for loneliness realted disease prescribing\n",
    "data['loneliness_perc'] = data['loneliness'].divide(data['ITEMS'], axis=0) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardisation (z-scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly aggregate percentages by postcodes by year.\n",
    "data['Year'] = data['Date'].dt.year\n",
    "# Aggregation\n",
    "cols = {'NUMBER_OF_PATIENTS': 'mean', 'SHA':'first', 'PCT':'first', 'oseast1m':'first', 'osnrth1m':'first',\n",
    "        'lsoa11': 'first', 'msoa11': 'first', 'ru11ind': 'first', 'rgn': 'first', 'laua':'first', 'imd': 'first',\n",
    "        'depression_perc': 'mean', 'alzheimers_perc': 'mean', 'blood pressure_perc': 'mean', 'hypertension_perc': 'mean',\n",
    "        'diabetes_perc': 'mean', 'cardiovascular disease_perc': 'mean', 'insomnia_perc': 'mean', 'addiction_perc': 'mean',\n",
    "        'social anxiety_perc': 'mean', 'loneliness_perc': 'mean'}\n",
    "\n",
    "data = data.groupby(['pcstrip','Year'], as_index=False).agg(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mean value returns a value broadly in the centre of the distribution of respective disease classes.\n",
    "# Therefore we'll go with an un-truncated arithmetic mean.\n",
    "# Can always revisit this assumption later.\n",
    "\n",
    "per_cols = ['depression_perc', 'alzheimers_perc', 'blood pressure_perc', 'hypertension_perc', \n",
    "            'diabetes_perc', 'cardiovascular disease_perc', 'insomnia_perc', 'addiction_perc',\n",
    "            'social anxiety_perc', 'loneliness_perc']\n",
    "\n",
    "# Get mean and std for baseline (2016)\n",
    "mean_std = data[data['Year'] == 2016][per_cols].agg(['mean','std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make new column names.\n",
    "std_cols = [col[:-4] + 'zscore' for col in per_cols]\n",
    "\n",
    "zscores = []    \n",
    "# z-score standardise for each year by baseline mean and std \n",
    "for year in [2016,2017,2018]:\n",
    "    zscores.append((data.loc[data['Year'] == year, per_cols] - mean_std.loc['mean', per_cols]) / mean_std.loc['std', per_cols])\n",
    "\n",
    "zscores = pd.concat(zscores).sort_index()\n",
    "data[std_cols] = zscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot zscores for loneliness\n",
    "f, [ax1, ax2, ax3] = plt.subplots(1,3, figsize = (14,6), sharey = True)\n",
    "\n",
    "# Note that there appears to be increasing variation in lonelines prescribing over time.\n",
    "# These means are comparable as standardised using 2016 mean and std.\n",
    "data[data['Year'] == 2016]['loneliness_zscore'].hist(bins=100, ax = ax1)\n",
    "data[data['Year'] == 2017]['loneliness_zscore'].hist(bins=100, ax = ax2)\n",
    "data[data['Year'] == 2018]['loneliness_zscore'].hist(bins=100, ax = ax3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save aggregated data\n",
    "data.to_csv(path[:-10] + \"processed_data_with_postcodes_GPs_stats.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Loneliness Variable\n",
    "\n",
    "The actual loneliness variable we work with is the sum of the standardised scores of: depression, alzheimers, hypertension, insomnia, addiction and social anxiety, for each year of interest.\n",
    "\n",
    "This means that the loneliness variable is actually an equally weighted index of the above domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum function ignores NAs\n",
    "data['loneills'] =  data[['depression_zscore', 'alzheimers_zscore', 'hypertension_zscore', 'insomnia_zscore',\n",
    "                          'addiction_zscore','social anxiety_zscore']].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot zscores for loneills\n",
    "f, [ax1, ax2, ax3] = plt.subplots(1,3, figsize = (14,6), sharey = True)\n",
    "\n",
    "# Note that there appears to be increasing variation in lonelines prescribing over time.\n",
    "data[data['Year'] == 2016]['loneills'].hist(bins=100, ax = ax1)\n",
    "data[data['Year'] == 2017]['loneills'].hist(bins=100, ax = ax2)\n",
    "data[data['Year'] == 2018]['loneills'].hist(bins=100, ax = ax3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save aggregated data\n",
    "data.to_csv(path[:-10] + \"final_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
